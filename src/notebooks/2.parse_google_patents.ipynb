{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a60306",
   "metadata": {},
   "source": [
    "## 对Google Patents进行解析\n",
    "\n",
    "- 格式为`JSON`\n",
    "- 对于标量类型的字段，直接作为主表字段\n",
    "- 对于一对多关系的复合类型字段，拆分成包含publication_id的分表\n",
    "- 对于多对多关系的复合类型字段，先查询实体ID，然后输出关系表\n",
    "- 部分冗余的脏字段可以抛弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = r'/data/dataset/google_patents'\n",
    "\n",
    "OUTPUT_BASE_PATH = r'/data/users/liangzhentao/Projects/google-patent-parser/data/output/'\n",
    "\n",
    "INVENTOR_HARMONIZED_PATH = OUTPUT_BASE_PATH + 'inventor_harmonized.csv'\n",
    "ASSIGNEE_HARMONIZED_PATH = OUTPUT_BASE_PATH + 'assignee_harmonized.csv'\n",
    "EXAMINER_PATH = OUTPUT_BASE_PATH + 'examiner.csv'\n",
    "\n",
    "OUTPUT_CONFIG = {\n",
    "    # 标量字段 (Scalar Fields)\n",
    "    'publication': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication.csv',\n",
    "        'headers': ['publication_number', 'application_number', 'country_code', 'kind_code', 'application_kind', 'pct_number', 'family_id', 'spif_publication_number', 'spif_application_number', 'publication_date', 'filing_date', 'grant_date', 'priority_date', 'entity_status', 'art_unit'],\n",
    "        'json_field': None  # 标量字段直接从根对象提取\n",
    "    },\n",
    "    # 一对多字段 (One-to-Many Fields)\n",
    "    'title': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_title.csv',\n",
    "        'headers': ['publication_number', 'text', 'language', 'truncated'],\n",
    "        'json_field': 'title_localized'\n",
    "    },\n",
    "    'abstract': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_abstract.csv',\n",
    "        'headers': ['publication_number', 'text', 'language', 'truncated'],\n",
    "        'json_field': 'abstract_localized'\n",
    "    },\n",
    "    'claims': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_claims.csv',\n",
    "        'headers': ['publication_number', 'text', 'language', 'truncated'],\n",
    "        'json_field': 'claims_localized'\n",
    "    },\n",
    "    'description': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_description.csv',\n",
    "        'headers': ['publication_number', 'text', 'language', 'truncated'],\n",
    "        'json_field': 'description_localized'\n",
    "    },\n",
    "    # 分类字段 (Classification Fields) - 结构类似，可以复用逻辑\n",
    "    'uspc': {'path': OUTPUT_BASE_PATH + 'publication_uspc.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'uspc'},\n",
    "    'ipc': {'path': OUTPUT_BASE_PATH + 'publication_ipc.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'ipc'},\n",
    "    'cpc': {'path': OUTPUT_BASE_PATH + 'publication_cpc.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'cpc'},\n",
    "    'fi': {'path': OUTPUT_BASE_PATH + 'publication_fi.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'fi'},\n",
    "    'fterm': {'path': OUTPUT_BASE_PATH + 'publication_fterm.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'fterm'},\n",
    "    'locarno': {'path': OUTPUT_BASE_PATH + 'publication_locarno.csv', 'headers': ['publication_number', 'code', 'inventive', 'first', 'tree'], 'json_field': 'locarno'},\n",
    "    # 多对多/复杂关系字段 (Many-to-Many / Complex Relations)\n",
    "    'inventor_map': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_inventor.csv',\n",
    "        'headers': ['publication_number', 'inventor_id', 'sequence'],  # 添加 'sequence'\n",
    "        'json_field': 'inventor_harmonized'\n",
    "    },\n",
    "    'assignee_map': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_assignee.csv',\n",
    "        'headers': ['publication_number', 'assignee_id', 'sequence'],  # 添加 'sequence'\n",
    "        'json_field': 'assignee_harmonized'\n",
    "    },\n",
    "    'examiner_map': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_examiner.csv',\n",
    "        'headers': ['publication_number', 'examiner_id'],\n",
    "        'json_field': 'examiner'\n",
    "    },\n",
    "    'patent_citation': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_citation.csv',\n",
    "        'headers': ['publication_number', 'cited_publication_number', 'type', 'category'], \n",
    "        'json_field': 'citation'\n",
    "    },\n",
    "    'non_patent_citation': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_non_patent_reference.csv',\n",
    "        'headers': ['publication_number', 'npl_text', 'type', 'category'],\n",
    "        'json_field': 'citation'\n",
    "    },\n",
    "    'priority_claim': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_priority_claim.csv',\n",
    "        'headers': ['publication_number', 'priority_application_number', 'filing_date'],\n",
    "        'json_field': 'priority_claim'\n",
    "    },\n",
    "    'child': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_child.csv',\n",
    "        'headers': ['publication_number', 'child_application_number', 'type', 'filing_date'], \n",
    "        'json_field': 'child'\n",
    "    },\n",
    "    'parent': {\n",
    "        'path': OUTPUT_BASE_PATH + 'publication_parent.csv',\n",
    "        'headers': ['publication_number', 'parent_application_number', 'type', 'filing_date'], \n",
    "        'json_field': 'parent'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson as json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f829a26",
   "metadata": {},
   "source": [
    "读取inventor、assignee、examiner的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading inventor ids...: 29452934it [00:59, 497972.26it/s]\n",
      "Reading assignee ids...: 17542829it [00:34, 508054.06it/s]\n",
      "Reading examiner ids...: 238057it [00:00, 390505.62it/s]\n"
     ]
    }
   ],
   "source": [
    "inventor_id_dict = {}\n",
    "with open(INVENTOR_HARMONIZED_PATH, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc='Reading inventor ids...'):\n",
    "        inventor_id_dict[f\"{row['name']}@@{row['country_code']}\"] = row['inventor_id']\n",
    "\n",
    "assignee_id_dict = {}\n",
    "with open(ASSIGNEE_HARMONIZED_PATH, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc='Reading assignee ids...'):\n",
    "        assignee_id_dict[f\"{row['name']}@@{row['country_code']}\"] = row['assignee_id']\n",
    "\n",
    "examiner_id_dict = {}\n",
    "with open(EXAMINER_PATH, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc='Reading examiner ids...'):\n",
    "        examiner_id_dict[f\"{row['name']}@@{row['department']}@@{row['level']}\"] = row['examiner_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c634b1",
   "metadata": {},
   "source": [
    "准备输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f83111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 13110 个文件需要处理\n",
      "首个文件名称 /data/dataset/google_patents/patents-000000000000\n",
      "最后文件名称 /data/dataset/google_patents/patents-000000013109\n"
     ]
    }
   ],
   "source": [
    "raw_jsonl_file_list = sorted(glob(RAW_DATA_PATH + '/*', recursive=True))\n",
    "print(f'共有 {len(raw_jsonl_file_list)} 个文件需要处理')\n",
    "print(f'首个文件名称 {raw_jsonl_file_list[0]}')\n",
    "print(f'最后文件名称 {raw_jsonl_file_list[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be8df9",
   "metadata": {},
   "source": [
    "准备输出函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batches(batches, initial_write=False):\n",
    "    \"\"\"将所有批处理数据写入对应的CSV文件。\"\"\"\n",
    "    for key, data in batches.items():\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        config = OUTPUT_CONFIG[key]\n",
    "        mode = 'w' if initial_write else 'a'\n",
    "        \n",
    "        with open(config['path'], mode, newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "            if initial_write:\n",
    "                writer.writerow(config['headers'])\n",
    "            writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee026f83",
   "metadata": {},
   "source": [
    "### 解析JSON文件并分批写入到结果CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 6/13110 [00:05<3:34:04,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# --- 4. 批量写入 ---\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line_counter % BATCH_SIZE == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     write_batches(batches, initial_write=is_first_write)\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# 清空所有批处理列表\u001b[39;00m\n\u001b[32m     96\u001b[39m     batches = {key: [] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m OUTPUT_CONFIG.keys()}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mwrite_batches\u001b[39m\u001b[34m(batches, initial_write)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initial_write:\n\u001b[32m     13\u001b[39m     writer.writerow(config[\u001b[33m'\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m writer.writerows(data)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000  # 每处理10000条记录就写入一次文件\n",
    "\n",
    "# 使用字典统一管理所有批处理列表\n",
    "batches = {key: [] for key in OUTPUT_CONFIG.keys()}\n",
    "is_first_write = True\n",
    "line_counter = 0\n",
    "\n",
    "for file in tqdm(raw_jsonl_file_list, desc='Processing files'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                patent = json.loads(line)\n",
    "                pub_num = patent.get('publication_number')\n",
    "                if not pub_num:\n",
    "                    continue\n",
    "\n",
    "                # --- 1. 处理标量字段 ---\n",
    "                scalar_row = [patent.get(h) for h in OUTPUT_CONFIG['publication']['headers']]\n",
    "                batches['publication'].append(scalar_row)\n",
    "\n",
    "                # --- 2. 处理通用的一对多关系 (Localized Text & Classifications) ---\n",
    "                one_to_many_keys = ['title', 'abstract', 'claims', 'description', \n",
    "                                    'uspc', 'ipc', 'cpc', 'fi', 'fterm', 'locarno']\n",
    "                for key in one_to_many_keys:\n",
    "                    config = OUTPUT_CONFIG[key]\n",
    "                    if config['json_field'] in patent:\n",
    "                        for item in patent[config['json_field']]:\n",
    "                            # 将tree字段（如果是列表）转换为字符串\n",
    "                            if 'tree' in item and isinstance(item.get('tree'), list):\n",
    "                                item['tree'] = ';'.join(item.get('tree', []))\n",
    "                            row = [pub_num] + [item.get(h) for h in config['headers'][1:]]\n",
    "                            batches[key].append(row)\n",
    "                \n",
    "                # --- 3. 处理复杂的多对多关系 ---\n",
    "                # Inventor (*** 已修改 ***)\n",
    "                if 'inventor_harmonized' in patent:\n",
    "                    # 使用 enumerate 来获取顺序号 (从0开始)\n",
    "                    for i, item in enumerate(patent['inventor_harmonized']):\n",
    "                        lookup_key = f\"{item.get('name')}@@{item.get('country_code')}\"\n",
    "                        inventor_id = inventor_id_dict.get(lookup_key)\n",
    "                        sequence = i + 1\n",
    "                        if inventor_id:\n",
    "                            # 在行末尾添加顺序号 i\n",
    "                            batches['inventor_map'].append([pub_num, inventor_id, sequence])\n",
    "\n",
    "                # Assignee (*** 已修改 ***)\n",
    "                if 'assignee_harmonized' in patent:\n",
    "                    # 使用 enumerate 来获取顺序号 (从0开始)\n",
    "                    for i, item in enumerate(patent['assignee_harmonized']):\n",
    "                        lookup_key = f\"{item.get('name')}@@{item.get('country_code')}\"\n",
    "                        assignee_id = assignee_id_dict.get(lookup_key)\n",
    "                        sequence = i + 1\n",
    "                        if assignee_id:\n",
    "                            # 在行末尾添加顺序号 i\n",
    "                            batches['assignee_map'].append([pub_num, assignee_id, sequence])\n",
    "\n",
    "                # Examiner\n",
    "                if 'examiner' in patent:\n",
    "                    for item in patent['examiner']:\n",
    "                        lookup_key = f\"{item.get('name')}@@{item.get('department')}@@{item.get('level')}\"\n",
    "                        examiner_id = examiner_id_dict.get(lookup_key)\n",
    "                        if examiner_id:\n",
    "                            batches['examiner_map'].append([pub_num, examiner_id])\n",
    "\n",
    "                # Citation (分为专利和非专利)\n",
    "                if 'citation' in patent:\n",
    "                    for item in patent['citation']:\n",
    "                        # 优先使用application_number，因为它更稳定\n",
    "                        cited_pub_num = item.get('publication_number')\n",
    "                        if cited_pub_num: # 认为是专利引用\n",
    "                            batches['patent_citation'].append([pub_num, cited_pub_num, item.get('type'), item.get('category')])\n",
    "                        elif item.get('npl_text'): # 认为是非专利引用\n",
    "                            batches['non_patent_citation'].append([pub_num, item.get('npl_text'), item.get('type'), item.get('category')])\n",
    "                \n",
    "                # Priority Claim\n",
    "                if 'priority_claim' in patent:\n",
    "                    for item in patent['priority_claim']:\n",
    "                        app_num = item.get('application_number')\n",
    "                        if app_num:\n",
    "                            batches['priority_claim'].append([pub_num, app_num, item.get('filing_date')])\n",
    "\n",
    "                # Child\n",
    "                if 'child' in patent:\n",
    "                    for item in patent['child']:\n",
    "                        app_num = item.get('application_number')\n",
    "                        if app_num:\n",
    "                             batches['child'].append([pub_num, app_num, item.get('type'), item.get('filing_date')])\n",
    "                \n",
    "                # Parent\n",
    "                if 'parent' in patent:\n",
    "                    for item in patent['parent']:\n",
    "                        app_num = item.get('application_number')\n",
    "                        if app_num:\n",
    "                             batches['parent'].append([pub_num, app_num, item.get('type'), item.get('filing_date')])\n",
    "\n",
    "                line_counter += 1\n",
    "                \n",
    "                # --- 4. 批量写入 ---\n",
    "                if line_counter % BATCH_SIZE == 0:\n",
    "                    write_batches(batches, initial_write=is_first_write)\n",
    "                    # 清空所有批处理列表\n",
    "                    batches = {key: [] for key in OUTPUT_CONFIG.keys()}\n",
    "                    is_first_write = False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # 增加异常捕获，避免单个错误行导致整个程序中断\n",
    "                print(f\"Error processing line: {line_counter}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 处理并写入最后一批剩余的数据 (Process and write the final remaining batch)\n",
    "# ==============================================================================\n",
    "write_batches(batches, initial_write=is_first_write)\n",
    "print(\"All files processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
